---
title : "Autoencoder (3)"
excerpt : "Denoising Autoencoder"
toc: true
toc_sticky: true
categories :
 - Deep Learning
tags:
 - AE


---



Autoencoder (AE)는 데이터를 압축하고 복원하기 위해 신경망을 사용하는 알고리즘으로, 입력 데이터를 저차원의 잠재 표현으로 압축하고(인코딩) 이러한 잠재 표현을 다시 입력 데이터와 유사한 출력으로 복원(디코딩)하는 과정을 한다.

AE 는 다양한 variation 이 있고, **Denoising Autoencoder (DAE)**도 그 중 하나이다.

SAE와 기본 AE와의 차이점은, DAE는 **노이즈가 있는 입력 데이터를 깨끗한 데이터로 복원**하는 목적을 갖는다. 

이를 위해 학습 과정에서 objective 는 **노이즈가 있는 데이터와 원본 데이터 간의 차이를 최소화**하도록 설정된다. 

이를 통해 모델은 데이터의 **중요한 특징을 포착하고 노이즈에 영향을 받지 않는** 방식으로 학습합니다.



DAE의 학습 과정은 다음과 같은 수식으로 표현된다

1. 노이즈가 있는 입력 데이터: $\mathbf{x}_n = \mathbf{x} + noise$, 
2. 인코더 함수: $h = f(\mathbf{x}_n)$, 여기서 h는 노이즈가 있는 데이터의 압축된 표현이다.
3. 디코더 함수: $\mathbf{x}' = g(h)$, 여기서 $x_r$는 노이즈가 있는 데이터로부터 복원된 데이터이다.
4. 손실 함수: $L(\mathbf{x},\mathbf{x}')= ||\mathbf{x}-\mathbf{x}'||^2$



### Code

```python
import torch
import torch.nn as nn

class DenoisingAutoencoder(nn.Module):
    def __init__(self, input_size, hidden_size):
        super(DenoisingAutoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_size, hidden_size),
            nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(hidden_size, input_size),
            nn.ReLU()
        )

    def forward(self, x_noisy):
        h = self.encoder(x_noisy)
        x_reconstructed = self.decoder(h)
# 학습
model.train()
for epoch in range(num_epochs):
    for data, _ in train_loader:
        x = data.view(-1, input_size)
        
        # 노이즈 추가
        noise = torch.rand(x.size()) * 0.5
        x_noisy = x + noise

        # 순전파 및 손실 계산
        x_reconstructed = model(x_noisy)
        loss = nn.MSELoss()(x, x_reconstructed)

        # 역전파 및 가중치 업데이트
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()


```

