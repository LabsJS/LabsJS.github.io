---
title : "Autoencoder (1)"
toc: true
toc_sticky: true
categories :
 - Deep Learning
tags:
 - AE

---



Autoencoders는 비지도 학습 방법으로 사용되는 인공신경망입니다. 주로 차원 축소와 특징 추출에 사용되며, 복잡한 데이터 구조를 간단한 형태로 압축하고 재구성하는 데 효과적입니다. 이 포스트에서는 Autoencoder의 기본 원리와 구조, 그리고 다양한 변형에 대해 알아보겠습니다.

### Autoencoders: Basic Structure

Autoencoder는 두 가지 주요 구성 요소인 인코더(encoder)와 디코더(decoder)로 구성되어 있습니다.

1. **인코더(Encoder)**: 인코더는 입력 데이터를 저차원의 잠재 공간(latent space)로 변환합니다. 인코더는 일반적으로 인공 신경망으로 구현되며, 다음과 같은 수식으로 표현할 수 있습니다.

   z = f(x; θ₁)

   여기서 x는 입력 데이터, z는 잠재 공간에 대한 표현이며, θ₁은 인코더의 가중치입니다. f는 비선형 활성화 함수를 포함한 인코더 네트워크입니다.

2. **디코더(Decoder)**: 디코더는 잠재 공간의 표현을 원래 입력 데이터와 유사한 데이터로 재구성합니다. 디코더 역시 인공 신경망으로 구현되며, 다음과 같은 수식으로 표현할 수 있습니다.

   x' = g(z; θ₂)

   여기서 x'는 재구성된 데이터, θ₂는 디코더의 가중치입니다. g는 비선형 활성화 함수를 포함한 디코더 네트워크입니다.

Autoencoder의 목표는 원본 입력 데이터 x와 재구성된 데이터 x' 사이의 차이를 최소화하는 것입니다. 이를 위해 손실 함수 L(x, x')를 정의하고, 이를 최소화하도록 인코더와 디코더의 가중치를 학습합니다.

L(x, x') = ||x - x'||²

이 손실 함수는 입력 데이터 x와 재구성된 데이터 x' 사이의 평균 제곱 오차(Mean Squared Error, MSE)를 최소화하는 것을 목표로 합니다. 이를 통해 autoencoder는 입력 데이터의 효과적인 압축 및 복원 방법을 학습하게 됩니다.

### Training an Autoencoder

Autoencoder의 학습은 비지도 방식으로 이루어집니다. 손실 함수를 사용하여 입력 데이터와 재구성된 출력 데이터 간의 차이를 최소화합니다. 일반적으로 평균제곱오차(mean squared error, MSE) 또는 교차 엔트로피(cross-entropy) 같은 손실 함수를 사용합니다.



### Variants of Autoencoders

기본 Autoencoder 외에도 여러 가지 변형이 존재합니다.

1. **Denoising Autoencoder**: 입력 데이터에 노이즈를 추가한 다음, 원본 데이터를 재구성하는 방식으로 학습합니다. 이를 통해 모델이 더 견고하게 특징을 추출할 수 있습니다.
2. **Sparse Autoencoder**: 학습 과정에서 몇 개의 뉴런만 활성화되도록 제약을 추가하여 희소한 표현을 학습합니다. 이를 통해 더 간결한 특징을 학습할 수 있습니다.
3. **Variational Autoencoder**: 인코더와 디코더 사이에 확률적 과정을 도입하여 생성 모델로 확장한 변형입니다. 데이터의 확률 분포를 학습하며, 새로운 데이터를 생성할 수 있습니다.

### Applications

Autoencoder는 다양한 분야에 활용됩니다.

1. **차원 축소**: 고차원 데이터를 저차원으로 압축하여 시각화하거나, 계산 효율을 높이는데 사용할 수 있습니다. 
2. **특징 추출**: Autoencoder는 비지도 학습을 통해 데이터의 중요한 특징을 학습할 수 있습니다. 이러한 특징은 다른 지도 학습 모델의 입력으로 사용되어 성능을 향상시킬 수 있습니다. 
3. **이상치 탐지**: Autoencoder는 정상 데이터에 대한 재구성 오차가 낮고, 이상 데이터에 대한 재구성 오차가 높은 경향이 있습니다. 이를 이용하여 이상치를 탐지할 수 있습니다.
4. **데이터 생성**: 변이형 Autoencoder와 같은 생성 모델을 사용하여 새로운 데이터를 생성할 수 있습니다. 이는 이미지 생성, 텍스트 생성 등 다양한 분야에 활용됩니다.

Conclusion

이 포스트에서는 Autoencoder의 기본 개념, 구조, 변형 및 활용 분야에 대해 알아보았습니다. Autoencoder는 차원 축소와 특징 추출을 위한 강력한 비지도 학습 도구로, 다양한 분야에서 유용하게 사용됩니다. 이를 통해 데이터의 효율적인 표현 및 처리가 가능해지며, 더 나은 모델 성능을 달성할 수 있습니다.



```python
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# 데이터셋 로드 (MNIST)
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)

# autoencoder 모델 정의
class Autoencoder(nn.Module):
    def __init__(self, input_size=784, latent_size=32):
        super(Autoencoder, self).__init__()
        self.encoder = nn.Sequential(nn.Linear(input_size, latent_size), nn.ReLU())
        self.decoder = nn.Sequential(nn.Linear(latent_size, input_size), nn.Sigmoid())

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = Autoencoder().to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 모델 학습
epochs = 50
for epoch in range(epochs):
    model.train()
    train_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        data = data.view(data.size(0), -1).to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, data)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()

    train_loss /= len(train_loader)
    print(f"Epoch: {epoch+1}/{epochs}, Loss: {train_loss:.6f}")

```

위 코드는 MNIST 데이터셋을 사용하여 간단한 완전 연결 계층(Fully-Connected Layer)으로 구성된 autoencoder를 구현합니다. 코드는 다음과 같은 단계로 이루어집니다.

1. **데이터셋 로드 및 전처리**: MNIST 데이터셋을 로드하고, 데이터를 정규화합니다. DataLoader를 사용하여 배치를 생성합니다.
2. **autoencoder 모델 정의**: 완전 연결 계층(nn.Linear)을 사용하여 인코더 및 디코더를 정의하는 Autoencoder 클래스를 정의합니다. 인코더는 입력 이미지를 잠재 공간으로 매핑하고, 디코더는 잠재 공간의 표현을 원래 이미지로 복원합니다.
3. **손실 함수 및 최적화 알고리즘 정의**: 손실 함수로 평균 제곱 오차(MSELoss)를 사용하고, 최적화 알고리즘으로 Adam을 사용합니다.
4. **모델 학습**: autoencoder를 학습시킵니다.

이 예제에서는 인코더와 디코더가 간단한 완전 연결 계층으로 구성되었지만, 실제 응용 프로그램에서는 더 복잡한 구조(예: 컨볼루션 레이어)를 사용하여 autoencoder를 구성할 수 있습니다.

모델 학습이 완료된 후에는 다음과 같이 테스트 데이터셋을 사용하여 autoencoder의 성능을 평가할 수 있습니다



```python
# 모델 평가
model.eval()
test_loss = 0
with torch.no_grad():
    for data, _ in test_loader:
        data = data.view(data.size(0), -1).to(device)
        output = model(data)
        loss = criterion(output, data)
        test_loss += loss.item()

test_loss /= len(test_loader)
print(f"Test Loss: {test_loss:.6f}")

```

또한 학습된 autoencoder를 사용하여 입력 이미지를 인코딩 및 디코딩하여 복원된 이미지를 시각화할 수 있습니다.



```python
import matplotlib.pyplot as plt

# 이미지 시각화
def visualize_images(images, title):
    fig, axes = plt.subplots(1, len(images), figsize=(10, 2))
    for i, ax in enumerate(axes):
        ax.imshow(images[i], cmap='gray')
        ax.set_xticks([])
        ax.set_yticks([])
    plt.suptitle(title)
    plt.show()

sample_data, _ = next(iter(test_loader))
sample_data = sample_data[:5].view(sample_data.size(0), -1).to(device)

with torch.no_grad():
    reconstructed_data = model(sample_data)

original_images = sample_data.cpu().view(-1, 28, 28).numpy()
reconstructed_images = reconstructed_data.cpu().view(-1, 28, 28).numpy()

visualize_images(original_images, "Original Images")
visualize_images(reconstructed_images, "Reconstructed Images")

```

이 코드는 테스트 데이터셋의 샘플 이미지를 시각화하고, autoencoder를 사용하여 복원된 이미지를 시각화합니다. 이를 통해 autoencoder의 성능을 직관적으로 확인할 수 있습니다. 이 예제는 간단한 완전 연결 계층을 사용한 autoencoder를 보여주지만, 필요에 따라 더 복잡한 구조로 변경할 수 있습니다.