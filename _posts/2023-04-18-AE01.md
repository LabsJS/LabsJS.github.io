---
title : "Autoencoder (1)"
excerpt : ""
toc: true
toc_sticky: true
categories :
 - Deep Learning
tags:
 - AE

---



Autoencoders는 비지도 학습 방법으로 사용되는 아키텍쳐이다. 

주로 차원 축소와 특징 추출에 사용되며, 복잡한 데이터 구조를 간단한 형태로 압축하고 재구성하는 데 효과적이다. 

본 문서에서는 Autoencoder의 기본 원리와 구조, 그리고 다양한 변형에 대해 다룬다.

### Autoencoders

Autoencoder (AE)는 인코더(encoder)와 디코더(decoder)로 구성된 네트워크이다.

1. **인코더(Encoder)**: 인코더는 입력 데이터를 저차원의 잠재 공간(latent space)로 변환한다. 
   인코더는 일반적으로 인공 신경망으로 구현되며, 다음과 같은 수식으로 표현할 수 있다.

   $\mathbf{z} = f (\mathbf{x};\theta_e)$

   여기서 $\mathbf{x}$는 입력 데이터, $\mathbf{z}$는 잠재 공간에 대한 표현이며, $\theta_e$은 인코더의 파라미터이다. (weight) 
   $f$ 는 비선형 활성화 함수를 포함한 인코더 네트워크이다.

2. **디코더(Decoder)**: 디코더는 잠재 공간의 표현을 원래 입력 데이터와 유사한 데이터로 재구성한다.
    디코더는 인공 신경망으로 구현되며, 다음과 같은 수식으로 표현할 수 있다.
   
   $\mathbf{x}' = g(\mathbf{z};\theta_d)$
   
   여기서 $\mathbf{x}'$는 재구성된 데이터, $\theta_d$는 디코더의 파라미터이다. 
   $g$는 비선형 활성화 함수를 포함한 디코더 네트워크이다.

AE의 objective는 원본 입력 데이터 $\mathbf{x}$와 재구성된 데이터 $\mathbf{x}'$ 사이의 차이를 최소화하는 것 이다. 
이를 위해 손실 함수 $L(\mathbf{x},\mathbf{x}')$를 정의하고, 이를 최소화하도록 인코더와 디코더의 가중치를 학습한다.



$L(\mathbf{x},\mathbf{x}') = ||\mathbf{x}-\mathbf{x}'||^2$

위 손실 함수는입력 데이터 $\mathbf{x}$와 재구성된 데이터 $\mathbf{x}'$ 사이의 평균 제곱 오차(Mean Squared Error, MSE)를 최소화하는 것을 objective 로 한 예시이다. 
이를 통해 AE는 입력 데이터의 효과적인 압축 및 복원 방법을 학습하게 된다.



### Variants of Autoencoders

1. **Denoising Autoencoder**: 입력 데이터에 노이즈를 추가한 다음, 원본 데이터를 재구성하는 방식으로 학습한다. 이를 통해 모델이 더 robust 하게 특징을 추출할 수 있다.
2. **Sparse Autoencoder**: 학습 과정에서 몇 개의 뉴런만 활성화되도록 제약을 추가하여 희소한 표현을 학습한다. 이를 통해 더 간결한 특징을 학습할 수 있다.
3. **Variational Autoencoder**: 인코더와 디코더 사이에 확률적 과정을 도입하여 생성 모델로 확장하였다. 
   데이터의 확률 분포를 학습하며, 새로운 데이터를 생성할 수 있다.



### Applications

1. **차원 축소**: 고차원 데이터를 저차원으로 압축하여 시각화하거나, 계산 효율을 높이는데 사용된다. 
2. **특징 추출**: AE는 비지도 학습을 통해 데이터의 중요한 특징을 학습할 수 있다. 이러한 특징은 다른 지도 학습 모델의 입력으로 사용되어 성능을 향상시킬 수 있다.
3. **이상치 탐지**: AE는 정상 데이터에 대한 재구성 오차가 낮고, 이상 데이터에 대한 재구성 오차가 높은 경향이 있다. 이를 이용하여 이상치를 탐지할 수 있다.
4. **데이터 생성**: Variational Autoencoder와 같은 생성 모델을 사용하여 새로운 데이터를 생성할 수 있다. 이는 이미지 생성, 텍스트 생성 등 다양한 분야에 활용되고 있다.

### Code

```python
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# 데이터셋 로드 (MNIST)
transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)

# autoencoder 모델 정의
class Autoencoder(nn.Module):
    def __init__(self, input_size=784, latent_size=32):
        super(Autoencoder, self).__init__()
        self.encoder = nn.Sequential(nn.Linear(input_size, latent_size), nn.ReLU())
        self.decoder = nn.Sequential(nn.Linear(latent_size, input_size), nn.Sigmoid())

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = Autoencoder().to(device)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# 모델 학습
epochs = 50
for epoch in range(epochs):
    model.train()
    train_loss = 0
    for batch_idx, (data, _) in enumerate(train_loader):
        data = data.view(data.size(0), -1).to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, data)
        loss.backward()
        optimizer.step()
        train_loss += loss.item()

    train_loss /= len(train_loader)
    print(f"Epoch: {epoch+1}/{epochs}, Loss: {train_loss:.6f}")

```


