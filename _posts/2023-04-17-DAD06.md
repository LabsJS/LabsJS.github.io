---
title : "Deep Anomaly Detection (6)"
excerpt : "Generic Normality Feature Learning - GAN"
toc: true
toc_sticky: true
categories :
 - Anomaly Detection
tags:
 - GAN

---

> 본 포스팅에서는 이상 탐지 관련 논문[^1] 을 참고 하여,
> Deep Anomaly Detection (DAD) 분야의 피쳐 추출 (feature extraction) 기법을 간략히 정리한다 



## Generic Normality Feature Learning: 

### Generative Adversarial Networks

이상 탐지에서는 GAN 기반 방식도 자주 사용된다. 

이 접근법은 일반적으로 데이터의 normality 를 잘 포착하는 latent 피쳐 공간을 생성 네트워크로 학습하는 것을 목표로 한다. 

실제 인스턴스와 생성된 인스턴스 간 residual 형태를 이상치 점수로 사용한다. 



#### Assumption

GAN 의 생성 네트워크에서 정산 데이터 인스턴스는 이상치보다 잠재 특징 공간에서 더 잘 생성될 수 있다. 



AnoGAN [^1]은 초기에 사용된 방법이다. 
데이터 인스턴스 x 가 주어지면 ,생성 네트워크 G의 학습된 latent 피쳐 공간에 따라 생성된 G(z) 와 최대한 비슷해지도록 한다. 

latent 공간이 학습 데이터의 기본 분포를 포착하도록 강제되므로, 이상 데이터는 정상 데이터보다 비슷하게 생성될 가능성이 낮다. 

GAN은 다음과 같은 objective 를 사용해 학습한다
$$
\min _{G} \max _{D} V(D, G)=\mathbb{E}_{\mathbf{x} \sim p_{X}}[\log D(\mathbf{x})]+\mathbb{E}_{\mathbf{z} \sim p_{Z}}[\log (1-D(G(\mathbf{z})))],
$$
여기서 G는 생성자(generator), D는 판별자(discriminator) 네트워크이고, 이들은 각각 $\Theta_G, \Theta_D$ 에 의해 파라미터화 된다.

V 는 two-player minmax game 의 value 함수이다. 

x 의 각각에 대해 최적의 z 를 찾기 위해서 2개의 loss 함수, residual, discrimination loss 를 사용한다. 

residual loss 는 다음과 같이 정의된다.
$$
\ell_{R}\left(\mathbf{x}, \mathbf{z}_{\gamma}\right)=\left\|\mathbf{x}-G\left(\mathbf{z}_{\gamma}\right)\right\|_{1},
$$
discrimination loss 는 다음과 같이 정의된다. (feature matching technique)
$$
\ell_{f m}\left(\mathbf{x}, \mathbf{z}_{\gamma}\right)=\left\|h(\mathbf{x})-h\left(G\left(\mathbf{z}_{\gamma}\right)\right)\right\|_{1},
$$
여기서 $\gamma$ 는 검색 반복 단계의 index 이고 h 는 D 의 중간 레이어로부터 매핑된 피쳐이다

검색은 무작위로 샘플된 z 로 부터 시작하여, 그레디언트 기반으로 z 를 업데이트 한다. 

전체 loss 는 각 term의 weight 에 해당하는 하이퍼파라미터 $\alpha$ 를 사용하여 다음과 같이 정의하고, 이는 이상 점수로도 사용된다. 
$$
s_{\mathbf{x}}=(1-\alpha) \ell_{R}\left(\mathbf{x}, \mathbf{z}_{\gamma^{*}}\right)+\alpha \ell_{f m}\left(\mathbf{x}, \mathbf{z}_{\gamma^{*}}\right)
$$


AnoGAN 의 주요 문제점 중 하나는, z 의 반복적 검색을 수행하는 과정에서 계산 효율성 낮아진다는 것이다.

이를 보완하기 위해 EBGAN [^2],fast AnoGAN [^3]에서는 데이터 인스턴스에서 latent 공간으로 매핑을 학습하는 추가 네트워크를 사용한다. 

여기서는 BiGAN [^4]을 기반으로 한 EBGAN을 중심으로 서술한다. 

BiGAN은 인코더를 사용해 x 를 z로 매핑하고, 동시에 인코더와 생성자, 판별자의 파라미터를 학습한다. 

BiGAN 은 x와 G(z) 를 구분하는 것 대신에, 인스턴스 (x,E(x)) 를 (G(z),z) 와 구분한다. 
$$
\min _{G, E} \max _{D} \quad \mathbb{E}_{\mathbf{x} \sim p_{X}}\left[\mathbb{E}_{\mathbf{z} \sim p_{E}(\cdot \mid \mathbf{x})} \log [D(\mathbf{x}, \mathbf{z})]\right]+\mathbb{E}_{\mathbf{z} \sim p_{Z}}\left[\mathbb{E}_{\mathbf{x} \sim p_{G}(\cdot \mid \mathbf{z})}[\log (1-D(\mathbf{x}, \mathbf{z}))]\right],
$$
이 경우에는 이상 점수를 다음과 같이 정의한다. 
$$
s_{\mathbf{X}}=(1-\alpha) \ell_{G}(\mathbf{x})+\alpha \ell_{D}(\mathbf{x}),
\newline
\ell_{G}(\mathbf{x})=\|\mathbf{x}-G(E(\mathbf{x}))\|_{1}, 
\newline \ell_{D}(\mathbf{x})=\|h(\mathbf{x}, E(\mathbf{x}))-h(G(E(\mathbf{x})), E(\mathbf{x}))\|_{1}
$$


EBGAN 은 ALAD 라는 방법으로 확장된다. 이 방식은 두개 이상의 판별자를 더해서, 하나의 판별자가 (x,x) 와 (x,G(E(x))) 사이를 구별하고, 다른 판별자는 (z,z) 와 (z,E(G(z))) 를 구별한다.

GANomaly [^5] 는 생성기 네트워크를 인코더-디코더-인코더 의 네트워크로 바꾸고, 두개의 손실 함수를 도입하여, 생성기를 개선하였다.

생성기는 다음과 같이 표현할 수 있다. $$\mathbf{x} \stackrel{G_{E}}{\longrightarrow} \mathbf{z} \stackrel{G_{D}}{\longrightarrow} \hat{\mathbf{x}} \stackrel{E}{\rightarrow} \hat{\mathbf{z}}$$ 

여기서 G 는 인코더 $G_E$와 디코더 $G_D$의 조합이다.
추가적으로 일반적으로 사용하는 feature matching loss 를 사용한다 
$$
\ell_{f m}=\mathbb{E}_{\mathbf{x} \sim p_{X}}\|h(\mathbf{x})-h(G(\mathbf{x}))\|_{2},
$$
생성기는 더 실제적인 instance 를 생성하기 위해서 contextual loss 와 encoding loss 를 포함한다
$$
\begin{array}{c}
\ell_{\text {con }}=\mathbb{E}_{\mathbf{x} \sim p_{X}}\|\mathbf{x}-G(\mathbf{x})\|_{1}, \\
\ell_{e n c}=\mathbb{E}_{\mathbf{x} \sim p_{X}}\left\|G_{E}(\mathbf{x})-E(G(\mathbf{x}))\right\|_{2} .
\end{array}
$$
Contexual loss 는 생성자가 입력 $x$ 가 $\hat{x}$ 을 생성할 때, contextual 정보를 고려하도록 제한다. 

Encodingloss 는 생성자가 생성된 인스턴스의 피쳐를 어떻게 인코드 하는지 학습하도록 한다.

전체 loss 는 다음과 같이 정의된다.
$$
\ell=\alpha \ell_{f m}+\beta \ell_{\text {con }}+\gamma \ell_{\text {enc }}
$$
여기서 $\alpha, \beta, \gamma$ 는각 loss 의 weight 에 대한 하이퍼 파라미터이다.

학습 데이터는 주로 정상 인스턴스를 포함하고 있기에 인코더 G 와 E 는 normal 인스턴스의 인코딩 쪽으로 최적화 되며, 따라서 이상 점수는 다음과 같이 정의할 수 있다.
$$
s_{\mathbf{x}}=\left\|G_{E}(\mathbf{x})-E(G(\mathbf{x}))\right\|_{1},
$$


여러 연구들에서 다양한 종류의 GAN이 제시되었으며 이러한 방법들이 이상 탐지의 성능을 향상 시킬 수 있다. 

#### Adavantages

1. 실제와 유사한 인스턴스를 생성하는 능력이 좋음, 이를 통해 재구성이 잘 안되는 이상 인스텀스를 탐지할 수 있음
2. 기존 많은 GAN 기반 모델과 이론을 적용 가능

#### Disadvantages

1. GAN 은 수렴 실패나 mode 붕괴 등으로 학습이 어려울 수 있음
2. 생성기가 잘못학습하여 정상 인스턴스 외의 데이터를 생성할 수 있음 (데이터 셋이 복잡하거나 이상치 포함되어 있는 경우 발생 확률 높음)
3. GAN 은 데이터 합성이 목적이라, 이상 탐지를 위한 최적의 모델은 아님



GAN 기반 이상 탐지 기술은 AE 와 마찬가지로, 학습된 저차원 latent 공간으로 부터의 재구성을 검사하여, 고차원의 이상치를 탐지할 수 있다. 이 공간이 이상 판별 정보를 보존한다면, 원본 데이터 공간보다 좋은 탐지 성능을 보일 수 있다. 



> Reference

[^1]:  Thomas Schlegl, Philipp Seeböck, Sebastian M Waldstein, Ursula Schmidt-Erfurth, and Georg Langs. 2017. Unsupervised anomaly detection with generative adversarial networks to guide marker discovery. In *IPMI*. Springer, Cham, 146–157.
[^2]: Houssam Zenati, Chuan Sheng Foo, Bruno Lecouat, Gaurav Manek, and Vijay Ramaseshan Chandrasekhar. 2018. Efficient gan-based anomaly detection. *arXiv preprint:1802.06222* (2018).
[^3]:Thomas Schlegl, Philipp Seeböck, Sebastian M Waldstein, Georg Langs, and Ursula Schmidt-Erfurth. 2019. f-AnoGAN: Fast unsuper vised anomaly detection with generative adversarial networks. *Medical Image Analysis* 54 (2019), 30–44.
[^4]:  Jeff Donahue, Philipp Krähenbühl, and Trevor Darrell. 2017. Adversarial feature learning. In *ICLR*. 

[^5]: Samet Akcay, Amir Atapour-Abarghouei, and Toby P Breckon. 2018. GANomaly: Semi-supervised anomaly detection via adversarial training. In *ACCV*. Springer, 622–637. 