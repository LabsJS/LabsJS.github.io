---
title : "Autoencoder (2)"
excerpt : "Sparse Autoencoder"
toc: true
toc_sticky: true
categories :
 - Deep Learning
tags:
 - AE
---





Autoencoder (AE)는 데이터를 압축하고 복원하기 위해 신경망을 사용하는 알고리즘으로, 입력 데이터를 저차원의 잠재 표현으로 압축하고(인코딩) 이러한 잠재 표현을 다시 입력 데이터와 유사한 출력으로 복원(디코딩)하는 과정을 한다.

AE 는 다양한 variation 이 있고, **Sparse Autoencoder (SAE)**도 그 중 하나이다.

SAE와 기본 AE와의 차이점은, **인코딩된 잠재 표현의 활성화를 제한하여 희소성(sparsity)을 높이는 방식**을 사용한다. 

이렇게 하면 네트워크가 데이터의 **중요한 특징을 포착하고 더욱 강력한 표현**을 배울 수 있다. 





SAE는 일반적으로 희소성을 도입하는 데 정규화를 사용하거나, 
특정 비율의 활성화를 0으로 만들어 제한하는 방식을 사용한다.

SAE 는 다음과 같은 objective 를 사용하여 희소성을 높인다.

$$L(\mathbf{x},\mathbf{x}') =L_r(\mathbf{x},\mathbf{x}') + \beta \cdot L_s(h) $$

여기서 L(x, x')는 복원 손실(reconstruction loss)로, 입력 x와 복원된 출력 x' 간의 차이를 측정한다. 일반적으로 MSE 나 cross-entropy loss를 사용할 수 있다.

(MSE 예시)

$$L_r(\mathbf{x},\mathbf{x}') = ||\mathbf{x}-\mathbf{x}'||^2$$

$h$ 는 인코딩된 잠재 표현(latent representation)이다. 

$L_s(h)$는 희소 손실(sparsity loss)로, 잠재 표현 $h$의 희소성을 측정한다. 

일반적으로 이를 위해 L1 정규화를 사용하거나, 특정 비율의 활성화를 0으로 만드는 방식(K-sparse autoencoder)을 사용할 수 있다.

$$L_s(h) = ||h||_1$$

여기서 β는 희소 손실의 가중치로, 희소성과 복원 손실 간의 균형을 조절한다.

SAE는 목적 함수 $L(\mathbf{x},\mathbf{x}')$를 최소화하도록 인코더와 디코더 네트워크의 파라미터를 학습한다. 

이를 통해 데이터의 중요한 특징을 포착하는 희소한 잠재 표현을 학습할 수 있다. 



### Code

```python
import torch
import torch.nn as nn
import torch.optim as optim

class SparseAutoencoder(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(SparseAutoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim, input_dim),
            nn.ReLU()
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return encoded, decoded
    
# 모델 생성
model = SparseAutoencoder(input_dim, hidden_dim)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# 학습 루프
for epoch in range(num_epochs):
    for data in dataloader:
        inputs, _ = data
        inputs = inputs.view(inputs.size(0), -1)

        # 순전파
        encoded, decoded = model(inputs)

        # 손실 계산
        reconstruction_loss = criterion(decoded, inputs)
        sparsity_loss = torch.norm(encoded, 1)
        loss = reconstruction_loss + beta * sparsity_loss

        # 역전파 및 가중치 업데이트
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

```





> 참고자료
>
> 1. Ng, A. (2011). Sparse autoencoder. CS294A Lecture notes, 72(2011), 1-19.
> 2. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.