---
title : "Autoencoder (2)"
excerpt : "Sparse Autoencoder"
toc: true
toc_sticky: true
categories :
 - Deep Learning
tags:
 - AE
---





Autoencoder는 데이터를 압축하고 복원하기 위해 신경망을 사용하는 알고리즘으로, 입력 데이터를 저차원의 잠재 표현으로 압축하고(인코딩) 이러한 잠재 표현을 다시 입력 데이터와 유사한 출력으로 복원(디코딩)하는 과정을 학습합니다.

Sparse Autoencoder는 기본 Autoencoder와의 차이점은, 인코딩된 잠재 표현의 활성화를 제한하여 희소성(sparsity)을 높이는 것입니다. 이렇게 하면 네트워크가 데이터의 중요한 특징을 포착하고 더욱 강력한 표현을 배울 수 있습니다. Sparse Autoencoder는 일반적으로 희소성을 도입하는 데 L1 정규화를 사용하거나, 특정 비율의 활성화를 0으로 만들어 제한하는 방식을 사용합니다.

Sparse Autoencoder의 주요 목적은 특징 추출 및 차원 축소입니다. 기본 Autoencoder와 마찬가지로, Sparse Autoencoder는 오토인코더가 효과적으로 입력 데이터의 패턴과 구조를 학습할 수 있게 해 주어, 비지도 사전 학습이나 차원 축소에 활용될 수 있습니다. 이러한 방식은 이미지 인식, 음성 인식, 자연어 처리 등 다양한 분야에서 사용되고 있습니다.

Sparse Autoencoder는 기본적인 Autoencoder에 비해 몇 가지 추가 요소가 있습니다. 여기에는 인코딩된 잠재 표현의 활성화를 제한하여 희소성을 높이는 손실 항이 포함됩니다. Sparse Autoencoder의 목적 함수는 다음과 같이 정의됩니다.

L(x, x') = L_r(x, x') + β * L_s(h)

여기서 L(x, x')는 복원 손실(reconstruction loss)로, 입력 x와 복원된 출력 x' 간의 차이를 측정합니다. 일반적으로 평균 제곱 오차(mean squared error) 또는 크로스 엔트로피 손실(cross-entropy loss)를 사용할 수 있습니다.

L_r(x, x') = ||x - x'||^2 (예를 들어, 평균 제곱 오차 사용)

h는 인코딩된 잠재 표현입니다. L_s(h)는 희소 손실(sparsity loss)로, 잠재 표현 h의 희소성을 측정합니다. 일반적으로 이를 위해 L1 정규화를 사용하거나, 특정 비율의 활성화를 0으로 만드는 방식(K-sparse autoencoder)을 사용할 수 있습니다.

L_s(h) = ||h||_1 (예를 들어, L1 정규화 사용)

여기서 β는 희소 손실의 가중치로, 희소성과 복원 손실 간의 균형을 조절합니다.

Sparse Autoencoder는 목적 함수 L(x, x')를 최소화하도록 인코더와 디코더 네트워크의 파라미터를 학습합니다. 이를 통해 데이터의 중요한 특징을 포착하는 희소한 잠재 표현을 학습할 수 있습니다. 이러한 희소한 표현은 차원 축소 및 비지도 사전 학습에 사용될 수 있습니다.



```python
import torch
import torch.nn as nn
import torch.optim as optim

class SparseAutoencoder(nn.Module):
    def __init__(self, input_dim, hidden_dim):
        super(SparseAutoencoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU()
        )
        self.decoder = nn.Sequential(
            nn.Linear(hidden_dim, input_dim),
            nn.ReLU()
        )

    def forward(self, x):
        encoded = self.encoder(x)
        decoded = self.decoder(encoded)
        return encoded, decoded

# 하이퍼파라미터 설정
input_dim = 784  # 예를 들어, MNIST 데이터셋의 경우 28x28 = 784
hidden_dim = 128
beta = 0.1
num_epochs = 100
learning_rate = 0.001

# 모델 생성
model = SparseAutoencoder(input_dim, hidden_dim)
criterion = nn.MSELoss()
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# 학습 데이터 로드 (예: MNIST 데이터셋)
# 여기에 데이터 로딩 코드를 추가해야 합니다.

# 학습 루프
for epoch in range(num_epochs):
    for data in dataloader:
        inputs, _ = data
        inputs = inputs.view(inputs.size(0), -1)

        # 순전파
        encoded, decoded = model(inputs)

        # 손실 계산
        reconstruction_loss = criterion(decoded, inputs)
        sparsity_loss = torch.norm(encoded, 1)
        loss = reconstruction_loss + beta * sparsity_loss

        # 역전파 및 가중치 업데이트
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

    print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))

```





> 참고자료
>
> 1. Ng, A. (2011). Sparse autoencoder. CS294A Lecture notes, 72(2011), 1-19.
> 2. Goodfellow, I., Bengio, Y., & Courville, A. (2016). Deep Learning. MIT Press.