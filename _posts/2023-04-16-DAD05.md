---
title : "Deep Anomaly Detection (5)"
excerpt : "Generic Normality Feature Learning - Autoencoder"
toc: true
toc_sticky: true
categories :
 - Anomaly Detection
tags:
 - AE

---

> 본 포스팅에서는 이상 탐지 관련 논문[^1] 을 참고 하여,
> Deep Anomaly Detection (DAD) 분야의 피쳐 추출 (feature extraction) 기법을 간략히 정리한다 





이 방법은 피쳐 학습과 이상치 점수를 결합하여, 이전(3) 과 같이 두 모듈을 분리하지는 않는다. 

일반적으로 이러한 방법은 generic 피쳐 학습과 anomaly measure-dependent 피쳐 학습 으로 나뉜다. 



## Generic Normality Feature Learning

이 방법은 일반적인(generic) 피쳐 학습 objective 함수를 최적화해서 데이터 인스턴스의 representation 을 학습한다.

학습된 representation 은 data regularity 들을 파악할 수 있기 때문에, 이상 탐지 성능을 높일 수 있다. 

형식적으로 이 프레임워크는 다음과 같이 표현할 수 있다

$$\{ \Theta^* ,\mathsf{W}^* \} = {\arg\limits_{\Theta,W}}\min \sum\limits_{\mathsf{x}\in \mathcal{X}} l (\psi (\phi (\mathsf{x};\Theta);\mathsf{W})),$$

$$s_\mathsf{x} = f (\mathsf{x}, \phi_{\Theta^*}, \psi_{\mathsf{W^*}})$$ 

여기서 $\phi$ 는 원본 데이터를 representation 공간 $\mathcal{Z}$ 에 매핑하는 함수이고, $\mathsf{W}$ 를 파라미터로 갖는 $\psi$ 는 $\mathcal{Z}$ 공간에서 작동하는 surrogate learning task 이다. 

이 task는 data regularity 의 학습을 강제하는데 사용된다. 

$l$ 은 기본 모델링 방식과 관련된 loss 함수 이며, $f$ 는 $\phi, \psi$ 를 사용해서 이상 점수 $s$ 를 구하기 위한 스코어링 함수이다. 

이 방식은 데이터 복원 (reconstruction), 생성 (genarative) 모델링과 예측 가능성  (predictability) 모델링, self-supervised learning 등 여러 관점에서 사용되는 방법을 포함한다.  



### Autoencoder (AE)

AE는 주어진 데이터 인스턴스가 잘 재구성 될 수 있는 저차원 피쳐 표현 공간을 학습하는 것을 목표로 한다.  이는 데이터 압축이나 차원 축소에서 많이 사용되는 기술이다. 

이 기술을 이상 탐지에 사용하는 휴리스틱은 학습된 특성 표현이 재구성 오류를 최소화 하기 위해 데이터의 중요한 regularity를 학습하도록 강제되며, 결과적으로 표현에서 이상치를 재구성하기 어렵고, 큰 재구성 에러를 갖게 된다. 

#### Assumptions

정상 인스턴스는 압축된 공간에서 더 잘 재구성 할 수 있다. 

AE 는 인코딩, 디코딩 네트워크로 구성된다. 인코더는 원래 데이터를 저차원 피쳐 공간에 매핑하고, 디코더는 프로젝트된 저차원 공간에서 데이터를 복구하려고 시도한다. 

이 두 네트워크의 파라미터는 재구성 loss 함수와 함께 학습된다. 

bottleneck 네트워크 아키텍처는 원래 데이터 보다 낮은 차원의 표현을 얻기 위해 사용되기도 하며, 이는 모델이 데이터 인스턴스를 재구성하는데 중요한 정보를 유지하도록 강제한다. 

전체 재구성 오류를 최소화 하기 위해 유지되는 정보는, 주요 인스턴스, 이상탐지에서는, 정상 인스턴스와 관련이 있어야 한다. 

결과적으로 이상 데이터는 재구성이 어렵게 된다. 그래서 재구성 오류를 이상치 점수로 직접 사용할 수 있다. 

이 방식의 기본 공식은 다음과 같다.
$$
\begin{array}{c}
\mathbf{z}=\phi_{e}\left(\mathbf{x} ; \Theta_{e}\right), \hat{\mathbf{x}}=\phi_{d}\left(\mathbf{z} ; \Theta_{d}\right), \\
\left\{\Theta_{e}^{*}, \Theta_{d}^{*}\right\}=\underset{\Theta_{e}, \Theta_{d}}{\arg \min } \sum_{\mathbf{x} \in \mathcal{X}}\left\|\mathbf{x}-\phi_{d}\left(\phi_{e}\left(\mathbf{x} ; \Theta_{e}\right) ; \Theta_{d}\right)\right\|^{2}, \\
s_{\mathbf{x}}=\left\|\mathbf{x}-\phi_{d}\left(\phi_{e}\left(\mathbf{x} ; \Theta_{e}^{*}\right) ; \Theta_{d}^{*}\right)\right\|^{2},
\end{array}
$$
여기서 $\phi_{e}$ 는 파라미터 $\Theta_e$ 를 가진 인코딩 네트워크이고 $\phi_d$ 는 파라미터 $\Theta_d$ 를 가진 디코딩 네트워크이다.

인코더와 디코더는 파라미터를 줄이고, 학습을 정규화 하기 위해 동일한 weight 파라미터를 공유할 수 있다. 여기서 $s$는 재구성 에러기반 anomaly 점수이다. 

여러 연구에서 다양한 종류의 정규화된 AE 가 도입되어 더 풍부하고 표현력있는 특성 표현을 학습할 수 있도록 했다. [1,2,3,4]



Sparse AE [^2] 의 경우 은닉층의 activation unit 에서 sparse 를 유도하도록 학습된다. 예를 들어 top-K 개의 active unit 을 유지한다.



Denosing AE [^5]는 원래 데이터가 아닌 사전에 정의된 corrupted 데이터 인스턴스에서 데이터를 복구하도록 학습하여, 표현이 작은 변화에도 robust 한 모델을 만들었다. 



Contractive AE [^3]는 인스턴스의 neighbor 의 작은 변화에 robust 하도록 학습한다. 이는 인코더의 activation 에 대한 자코비안 행렬의 Frobenious norm 을 기반으로 패널티를 추가하여 학습한다. 



Varitional AE [^1]는 표현 공간에 정규화를 적용하여, 데이터 인스턴스를 잠재 공간에 대한 prior 분포를 사용하여 인코딩한다. 이를 통해 오버피팅을 방지하고, 의미있는 데이터 인스턴스를 생성할 수 있는 공간의 좋은 속성을 보장한다.

오토인코더는 구현하기 쉽고, 이상 탐지에 직관적인 방법이다. 이는 

Replicator neural network [^5] 은 static 다차원/표 데이터에 중점을 둔 실험과 함께, 데이터 재구성 아이디어를 처음으로 제시한 연구이다. 

이 네트워크는 MLP를 기반으로, 3개의 은닉층을 갖는다. tanh 함수를 사용하여, 다른 입력 값에 대해 다른 활성화 수준을 얻으며, 이를 통해 중간 표현을 사전에 정의된 구간으로 이산화 한다. 

결과적으로 은닉층은 데이터 인스턴스를 여러 그룹으로 클러스터링하게 되고, 이를 통해 군집화된 이상치를 감지할 수 있다. 



RandNet [^6]은 AE앙상블을 적용하였다. 
독립적인 AE 세트를 학습시키고, 각 AE 는 일부 랜덤으로 선택된 드롭아웃을 적용한다. exponential 하게 batch 의 샘플의 크기를 늘리는 샘플링 방식이 사용된다. 
여기서는 tabular 데이터를 중점으로 두었다.



AE 는 표 외에 시퀀스 데이터, 그래프, 이미지/비디오 데이터 같은 경우에도 적용된다. 

일반적으로 복잡한 데이터에 대한 AE 적용은 두가지 유형이 있다. 

1.  AE 의 기존 절차를 따르면서, 입력 데이터의 유형을 맞게 조정하는 방법 
   e.g. CNN-AE, LSTM-AE, Conv-LSTM-AE, GCN-AE 등
2. (1) AE를 먼저 사용하여, 저차원 표현을 학습한 후에, (2)이를 사용해 예측하는 방법

 [^7] 에서는 Denoise AE 를 RNN 과 결합하였다. 
각 시간 단계에서 다차원 데이터 입력의 표현을 학습하고, 이를 RNN 의 입력으로 넣어서 사용했다.



#### Advantages

1. 다양한 데이터 유형에 대해 일반적이며, 직관적이다
2. 다양한 종류의 AE variation 들을 사용할 수 있다

#### Disadvantages

1. 학습된 피쳐 표현은 sparse 한 regularity 와 학습 데이터의 이상치로 인해 bias 가 발생할 수 있다.
2. 재구성의 objective fuction 은 차원 축소나 데이터 압축을 위해 설계되어, 이상치 감지를 위한 것이 아니다.
   다시말해, 불규칙성을 감지하기 위해 최적화 된 방식이 아니다. 





AE 프레임워크에서 다양한 종류의 신경망 레이어와 아키텍처를 사용할 수 있어서, 고차원 데이터에서 이상치를 감지하고, graph 나 다변량 sequence 데이터와 같은 데이터에서도 이상치를 감지할 수 있다. 
학습된 표현이 표현력이 좋으면, 기존의 hand-cragt 피쳐 기반 방식보다 false positive 결과를 줄일 수 있다. 

AE 는 일반적으로 학습데이터에 존재하는 노이즈에 취약하며, 그러한 노이즈를 기억하도록 학습될 수 있어서, 심한 overfitting 과 이상치의 작은 재구성 에러를 발생시킬 수 있다. 



> References

[^1]: Carl Doersch. 2016. Tutorial on variational autoencoders. arXiv preprint:1606.05908 (2016).
[^2]: Alireza Makhzani and Brendan Frey. 2014. K-sparse autoencoders. In ICLR. 
[^3]: Salah Rifai, Pascal Vincent, Xavier Muller, Xavier Glorot, and Yoshua Bengio. 2011. Contractive auto-encoders: explicit invariance during feature extraction. In ICML. 833–840.
[^4]: Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre-Antoine Manzagol. 2010. Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion. Journal of Machine Learning Research 11, Dec (2010), 3371–3408.
[^5]:  Simon Hawkins, Hongxing He, Graham Williams, and Rohan Baxter. 2002. Outlier detection using replicator neural networks. In DaWaK.
[^6]: Jinghui Chen, Saket Sathe, Charu Aggarwal, and Deepak Turaga. 2017. Outlier detection with autoencoder ensembles. In SDM. 90–98.
[^7]: Weining Lu, Yu Cheng, Cao Xiao, Shiyu Chang, Shuai Huang, Bin Liang, and Thomas Huang. 2017. Unsupervised sequential outlier detection with deep architectures. IEEE Transactions on Image Processing 26, 9 (2017), 4321–4330.
