Autoencoder (AE)는 데이터를 압축하고 복원하기 위해 신경망을 사용하는 알고리즘으로, 입력 데이터를 저차원의 잠재 표현으로 압축하고(인코딩) 이러한 잠재 표현을 다시 입력 데이터와 유사한 출력으로 복원(디코딩)하는 과정을 한다.

AE 는 다양한 variation 이 있고, **Variational Autoencoder (VAE)**도 그 중 하나이다.

VAE의 는 기본적인 오토인코더와 비슷한 구조를 가지지만 생성 모델으로, 
잠재 공간에서 확률 분포를 학습하며, 이를 통해 실제 데이터와 유사한 새로운 데이터를 생성할 수 있다. 때문에 ML 커뮤니티에서 많은 관심을 받고 있다. 



다음과 같이 수식으로 설명할 수 있다

인코더:

$q(z \mid x) \approx N\left(z ; \mu, \sigma^2 \right)$

인코더는 데이터 x를 받아 잠재 변수 z의 조건부 확률 분포 $q(z|x)$를 근사한다. 

인코더의 출력은 평균 벡터 $\mu$, 와 로그 분산 벡터 $\log\sigma^2$가 있다.

디코더:

디코더는 잠재 변수 z를 받아 데이터 x의 조건부 확률 분포 $p(x|z)$를 근사한다. 
이 분포는 데이터를 재구성하는 데 사용된다.

변분 추론:

변분 추론은 실제 posteri p(z|x)를 인코더의 출력인 q(z|x)로 근사하는 과정입니다. 이를 위해 쿨백-라이블러 발산(Kullback-Leibler Divergence, KL Divergence)을 최소화하는 방식으로 인코더의 출력을 조절합니다.

KL(q(z|x) || p(z)) = ∫q(z|x)log(q(z|x)/p(z))dz

p(z)는 잠재 변수 z의 사전 분포로, 보통 표준 정규 분포 N(0, I)를 사용합니다.

4. 재구성 손실:

재구성 손실은 디코더가 생성한 x의 확률 분포 p(x|z)와 실제 데이터 x 사이의 차이를 측정합니다. 이를 최소화함으로써 입력 데이터 x를 잘 재구성하도록 디코더를 학습합니다.

L_recon = -∑xlog(p(x|z))

5. 최종 목표 함수:

VAE의 목표 함수는 쿨백-라이블러 발산(KL Divergence) 항과 재구성 손실 항을 결합한 것입니다. 이 목표 함수를 최소화하여 모델을 최적화합니다.

L = L_recon + KL(q(z|x) || p(z))

이렇게 설명한 VAE의 수식은 인코더와 디코더를 사용하여 데이터의 의미있는 잠재 표현을 학습하고, 이를 바탕으로 데이터를 재